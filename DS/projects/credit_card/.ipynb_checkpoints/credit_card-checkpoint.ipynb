{"cells":[{"source":"Commercial banks receive _a lot_ of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual's credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this notebook, we will build an automatic credit card approval predictor using machine learning techniques, just like real banks do.\n\n![Credit card being held in hand](credit_card.jpg)\n\nYou have been provided with a small subset of the credit card applications a bank receives. The dataset has been loaded as a Pandas DataFrame for you. You will start from there. ","metadata":{},"id":"35aebf2e-0635-4fef-bc9a-877b6a20fb13","cell_type":"markdown"},{"source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \ncc_apps.head()","metadata":{"executionCancelledAt":null,"executionTime":369,"lastExecutedAt":1705286074017,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \ncc_apps.head()","outputsMetadata":{"0":{"height":216,"type":"dataFrame"}}},"id":"6e86b1e8-a3fa-4b09-982f-795f218bd1a6","cell_type":"code","execution_count":2,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"0","type":"string"},{"name":"1","type":"string"},{"name":"2","type":"number"},{"name":"3","type":"string"},{"name":"4","type":"string"},{"name":"5","type":"string"},{"name":"6","type":"string"},{"name":"7","type":"number"},{"name":"8","type":"string"},{"name":"9","type":"string"},{"name":"10","type":"integer"},{"name":"11","type":"string"},{"name":"12","type":"string"},{"name":"13","type":"string"},{"name":"14","type":"integer"},{"name":"15","type":"string"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"0":["b","a","a","b","b"],"1":["30.83","58.67","24.50","27.83","20.17"],"2":[0,4.46,0.5,1.54,5.625],"3":["u","u","u","u","u"],"4":["g","g","g","g","g"],"5":["w","q","q","w","w"],"6":["v","h","h","v","v"],"7":[1.25,3.04,1.5,3.75,1.71],"8":["t","t","t","t","t"],"9":["t","t","f","t","f"],"10":[1,6,0,5,0],"11":["f","f","f","t","f"],"12":["g","g","g","g","s"],"13":["00202","00043","00280","00100","00120"],"14":[0,560,824,3,0],"15":["+","+","+","+","+"],"index":[0,1,2,3,4]}},"total_rows":5,"truncation_type":null},"text/plain":"  0      1      2  3  4  5  6     7  8  9   10 11 12     13   14 15\n0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  f  g  00202    0  +\n1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  f  g  00043  560  +\n2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  f  g  00280  824  +\n3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  t  g  00100    3  +\n4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  f  s  00120    0  +","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b</td>\n      <td>30.83</td>\n      <td>0.000</td>\n      <td>u</td>\n      <td>g</td>\n      <td>w</td>\n      <td>v</td>\n      <td>1.25</td>\n      <td>t</td>\n      <td>t</td>\n      <td>1</td>\n      <td>f</td>\n      <td>g</td>\n      <td>00202</td>\n      <td>0</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a</td>\n      <td>58.67</td>\n      <td>4.460</td>\n      <td>u</td>\n      <td>g</td>\n      <td>q</td>\n      <td>h</td>\n      <td>3.04</td>\n      <td>t</td>\n      <td>t</td>\n      <td>6</td>\n      <td>f</td>\n      <td>g</td>\n      <td>00043</td>\n      <td>560</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a</td>\n      <td>24.50</td>\n      <td>0.500</td>\n      <td>u</td>\n      <td>g</td>\n      <td>q</td>\n      <td>h</td>\n      <td>1.50</td>\n      <td>t</td>\n      <td>f</td>\n      <td>0</td>\n      <td>f</td>\n      <td>g</td>\n      <td>00280</td>\n      <td>824</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b</td>\n      <td>27.83</td>\n      <td>1.540</td>\n      <td>u</td>\n      <td>g</td>\n      <td>w</td>\n      <td>v</td>\n      <td>3.75</td>\n      <td>t</td>\n      <td>t</td>\n      <td>5</td>\n      <td>t</td>\n      <td>g</td>\n      <td>00100</td>\n      <td>3</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b</td>\n      <td>20.17</td>\n      <td>5.625</td>\n      <td>u</td>\n      <td>g</td>\n      <td>w</td>\n      <td>v</td>\n      <td>1.71</td>\n      <td>t</td>\n      <td>f</td>\n      <td>0</td>\n      <td>f</td>\n      <td>s</td>\n      <td>00120</td>\n      <td>0</td>\n      <td>+</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":2}]},{"source":"# Start coding here\n# Use as many cells as you need\ncc_apps.describe()","metadata":{"executionCancelledAt":null,"executionTime":27,"lastExecutedAt":1705286078769,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Start coding here\n# Use as many cells as you need\ncc_apps.describe()"},"id":"adc05611-4f58-4d58-a767-ca973e3cc331","cell_type":"code","execution_count":3,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"string"},{"name":"2","type":"number"},{"name":"7","type":"number"},{"name":"10","type":"number"},{"name":"14","type":"number"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"2":[690,4.7587246377,4.9781632485,0,1,2.75,7.2075,28],"7":[690,2.2234057971,3.3465133593,0,0.165,1,2.625,28.5],"10":[690,2.4,4.8629400342,0,0,0,3,67],"14":[690,1017.3855072464,5210.1025983027,0,0,5,395.5,100000],"index":["count","mean","std","min","25%","50%","75%","max"]}},"total_rows":8,"truncation_type":null},"text/plain":"               2           7          10             14\ncount  690.000000  690.000000  690.00000     690.000000\nmean     4.758725    2.223406    2.40000    1017.385507\nstd      4.978163    3.346513    4.86294    5210.102598\nmin      0.000000    0.000000    0.00000       0.000000\n25%      1.000000    0.165000    0.00000       0.000000\n50%      2.750000    1.000000    0.00000       5.000000\n75%      7.207500    2.625000    3.00000     395.500000\nmax     28.000000   28.500000   67.00000  100000.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>2</th>\n      <th>7</th>\n      <th>10</th>\n      <th>14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>690.000000</td>\n      <td>690.000000</td>\n      <td>690.00000</td>\n      <td>690.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>4.758725</td>\n      <td>2.223406</td>\n      <td>2.40000</td>\n      <td>1017.385507</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>4.978163</td>\n      <td>3.346513</td>\n      <td>4.86294</td>\n      <td>5210.102598</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n      <td>0.165000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.750000</td>\n      <td>1.000000</td>\n      <td>0.00000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.207500</td>\n      <td>2.625000</td>\n      <td>3.00000</td>\n      <td>395.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>28.000000</td>\n      <td>28.500000</td>\n      <td>67.00000</td>\n      <td>100000.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":3}]},{"source":"# Drop the features 11 and 13\ncc_apps = cc_apps.drop([11, 13], axis=1)\n\n# Split into train and test sets\ncc_apps_train, cc_apps_test = train_test_split(cc_apps, test_size=0.33, random_state=42)\n\n# Replace the '?'s with NaN in the train and test sets\ncc_apps_train_nans_replaced = cc_apps_train.replace(\"?\", np.NaN)\ncc_apps_test_nans_replaced = cc_apps_test.replace(\"?\", np.NaN)\n\n# Impute the missing values with mean imputation\ncc_apps_train_imputed = cc_apps_train_nans_replaced.fillna(cc_apps_train_nans_replaced.mean())\ncc_apps_test_imputed = cc_apps_test_nans_replaced.fillna(cc_apps_train_nans_replaced.mean())\n\n# Iterate over each column of cc_apps_train_imputed\nfor col in cc_apps_train_imputed.columns:\n    # Check if the column is of object type\n    if cc_apps_train_imputed[col].dtypes == \"object\":\n        # Impute with the most frequent value\n        cc_apps_train_imputed = cc_apps_train_imputed.fillna(\n            cc_apps_train_imputed[col].value_counts().index[0]\n        )\n        cc_apps_test_imputed = cc_apps_test_imputed.fillna(\n            cc_apps_train_imputed[col].value_counts().index[0]\n        )\n\n# Convert the categorical features in the train and test sets independently\ncc_apps_train_cat_encoding = pd.get_dummies(cc_apps_train_imputed)\ncc_apps_test_cat_encoding = pd.get_dummies(cc_apps_test_imputed)\n\n# Reindex the columns of the test set aligning with the train set\ncc_apps_test_cat_encoding = cc_apps_test_cat_encoding.reindex(\n    columns=cc_apps_train_cat_encoding.columns, fill_value=0\n)\n\n# Segregate features and labels into separate variables\nX_train, y_train = (\n    cc_apps_train_cat_encoding.iloc[:, :-1].values,\n    cc_apps_train_cat_encoding.iloc[:, [-1]].values,\n)\nX_test, y_test = (\n    cc_apps_test_cat_encoding.iloc[:, :-1].values,\n    cc_apps_test_cat_encoding.iloc[:, [-1]].values,\n)\n\n# Instantiate MinMaxScaler and use it to rescale X_train and X_test\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)\n\n# Instantiate a LogisticRegression classifier with default parameter values\nlogreg = LogisticRegression()\n\n# Fit logreg to the train set\nlogreg.fit(rescaledX_train, y_train)\n\n# Use logreg to predict instances from the test set and store it\ny_pred = logreg.predict(rescaledX_test)\n\n# Print the confusion matrix of the logreg model\nprint(confusion_matrix(y_test, y_pred))\n\n# Define the grid of values for tol and max_iter\ntol = [0.01, 0.001, 0.0001]\nmax_iter = [100, 150, 200]\n\n# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\nparam_grid = dict(tol=tol, max_iter=max_iter)\n\n# Instantiate GridSearchCV with the required parameters\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n\n# Fit grid_model to the data\ngrid_model_result = grid_model.fit(rescaledX_train, y_train)\n\n# Summarize results\nbest_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_score, best_params))\n\n# Extract the best model and evaluate it on the test set\nbest_model = grid_model_result.best_estimator_\nprint(\n    \"Accuracy of logistic regression classifier: \",\n    best_model.score(rescaledX_test, y_test),\n)","metadata":{},"cell_type":"code","id":"dd85f96f-e45e-4c63-bdd7-f2e658ccec70","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}