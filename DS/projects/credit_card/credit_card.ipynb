{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190b9139-b21f-4470-b9d9-daf4a1249e95",
   "metadata": {},
   "source": [
    "# Predicting Credit Card Approvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aebf2e-0635-4fef-bc9a-877b6a20fb13",
   "metadata": {},
   "source": [
    "Commercial banks receive _a lot_ of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual's credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this notebook, we will build an automatic credit card approval predictor using machine learning techniques, just like real banks do.\n",
    "\n",
    "![Credit card being held in hand](credit_card.jpg)\n",
    "\n",
    "You have been provided with a small subset of the credit card applications a bank receives. The dataset has been loaded as a Pandas DataFrame for you. You will start from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e86b1e8-a3fa-4b09-982f-795f218bd1a6",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 369,
    "lastExecutedAt": 1705286074017,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \ncc_apps.head()",
    "outputsMetadata": {
     "0": {
      "height": 216,
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "0": [
          "b",
          "a",
          "a",
          "b",
          "b"
         ],
         "1": [
          "30.83",
          "58.67",
          "24.50",
          "27.83",
          "20.17"
         ],
         "10": [
          1,
          6,
          0,
          5,
          0
         ],
         "11": [
          "f",
          "f",
          "f",
          "t",
          "f"
         ],
         "12": [
          "g",
          "g",
          "g",
          "g",
          "s"
         ],
         "13": [
          "00202",
          "00043",
          "00280",
          "00100",
          "00120"
         ],
         "14": [
          0,
          560,
          824,
          3,
          0
         ],
         "15": [
          "+",
          "+",
          "+",
          "+",
          "+"
         ],
         "2": [
          0,
          4.46,
          0.5,
          1.54,
          5.625
         ],
         "3": [
          "u",
          "u",
          "u",
          "u",
          "u"
         ],
         "4": [
          "g",
          "g",
          "g",
          "g",
          "g"
         ],
         "5": [
          "w",
          "q",
          "q",
          "w",
          "w"
         ],
         "6": [
          "v",
          "h",
          "h",
          "v",
          "v"
         ],
         "7": [
          1.25,
          3.04,
          1.5,
          3.75,
          1.71
         ],
         "8": [
          "t",
          "t",
          "t",
          "t",
          "t"
         ],
         "9": [
          "t",
          "t",
          "f",
          "t",
          "f"
         ],
         "index": [
          0,
          1,
          2,
          3,
          4
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "integer"
          },
          {
           "name": "0",
           "type": "string"
          },
          {
           "name": "1",
           "type": "string"
          },
          {
           "name": "2",
           "type": "number"
          },
          {
           "name": "3",
           "type": "string"
          },
          {
           "name": "4",
           "type": "string"
          },
          {
           "name": "5",
           "type": "string"
          },
          {
           "name": "6",
           "type": "string"
          },
          {
           "name": "7",
           "type": "number"
          },
          {
           "name": "8",
           "type": "string"
          },
          {
           "name": "9",
           "type": "string"
          },
          {
           "name": "10",
           "type": "integer"
          },
          {
           "name": "11",
           "type": "string"
          },
          {
           "name": "12",
           "type": "string"
          },
          {
           "name": "13",
           "type": "string"
          },
          {
           "name": "14",
           "type": "integer"
          },
          {
           "name": "15",
           "type": "string"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 5,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.25</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00202</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.460</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>3.04</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>6</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00043</td>\n",
       "      <td>560</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>24.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>1.50</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00280</td>\n",
       "      <td>824</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.540</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>3.75</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00100</td>\n",
       "      <td>3</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>20.17</td>\n",
       "      <td>5.625</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.71</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>00120</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  0      1      2  3  4  5  6     7  8  9   10 11 12     13   14 15\n",
       "0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  f  g  00202    0  +\n",
       "1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  f  g  00043  560  +\n",
       "2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  f  g  00280  824  +\n",
       "3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  t  g  00100    3  +\n",
       "4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  f  s  00120    0  +"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the dataset\n",
    "cc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \n",
    "cc_apps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8472f7-219e-4b93-9466-dd8a18484367",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 27,
    "lastExecutedAt": 1705286078769,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Start coding here\n# Use as many cells as you need\ncc_apps.describe()"
   },
   "source": [
    "# Start coding here\n",
    "# Use as many cells as you need\n",
    "cc_apps.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae29dcd9-b347-4690-8fb5-a4812f5269d9",
   "metadata": {},
   "source": [
    "### How to approach this project?\n",
    "\n",
    "1. Splitting the dataset into train and test sets\n",
    "\n",
    "2. Handling the missing values\n",
    "\n",
    "3. Preprocessing the data\n",
    "\n",
    "4. Segregating features and labels and feature rescaling\n",
    "\n",
    "5. Training and evaluating a logistic regression model\n",
    "\n",
    "6. Hyperparameter search and making the model perform better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d28476-0b6d-455e-a297-042272fb0edf",
   "metadata": {},
   "source": [
    "### 1. Splitting the dataset into train and test sets\n",
    "\n",
    "    Drop features 11 and 13 using the drop() method as they are non-essential for the task of this project.\n",
    "    \n",
    "    Using the train_test_split() method, split the data into train and test sets with a split ratio of 33% (test_size argument) and set the random_state argument to 42. \n",
    "    \n",
    "    Assign the train and test DataFrames to the following variables respectively: cc_apps_train, cc_apps_test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b576d1-0ab4-4b3f-924b-b3659cf3ee42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9be2c47-a053-4227-a0ac-cec9bd17e058",
   "metadata": {},
   "source": [
    "### 2. Handling the missing values\n",
    "\n",
    "Replace '?' with NaNs using replace() in both train and test sets. Store them in cc_apps_train_nans_replaced and cc_apps_test_nans_replaced respectively.\n",
    "Impute missing values (NaNs) in numeric columns with fillna(), ensuring test set imputation uses mean values from the training set. Store the imputed DataFrames in cc_apps_train_imputed and cc_apps_test_imputed respectively.\n",
    "Iterate through cc_apps_train_imputed columns with a for loop, checking for object data type.\n",
    "Impute missing values in both cc_apps_train_imputed and cc_apps_test_imputed columns with the most frequent value from cc_apps_train_imputed using fillna() and value_counts().\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e3aee7-9cdf-4c66-bab7-6df451a20934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6657b93e-29a1-4fa4-a523-1ae3975bc719",
   "metadata": {},
   "source": [
    "### 3. Preprocessing the data\n",
    "\n",
    "Apply get_dummies() to cc_apps_train_imputed and cc_apps_test_imputed, storing the results in cc_apps_train_cat_encoding and cc_apps_test_cat_encoding, respectively.\n",
    "Reindex cc_apps_test_cat_encoding with columns from cc_apps_train_cat_encoding, filling missing columns with 0s.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa885a7-dd9c-4fc6-8ee7-a958d89d0c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e4f4deb-4f9e-4995-9234-ec78b51718ab",
   "metadata": {},
   "source": [
    "### 4. Segregating features and labels and feature rescaling\n",
    "\n",
    "Segregate cc_apps_train_cat_encoding into (X_train, y_train) and cc_apps_test_cat_encoding into (X_test, y_test).\n",
    "Instantiate a MinMaxScaler as scaler with feature_range set to (0,1).\n",
    "Fit scaler to X_train and transform data, saving it as rescaledX_train.\n",
    "Transform X_test using scaler and store the result in rescaledX_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46abbc5-748d-4d9a-a6d9-76bea43780a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2b4d8f8-44ed-4712-9409-84834f69eeee",
   "metadata": {},
   "source": [
    "### 5. Training and evaluating a logistic regression model\n",
    "\n",
    "Instantiate LogisticRegression into a variable named logreg with default values.\n",
    "Train logreg on rescaledX_train and y_train.\n",
    "Make predictions on rescaledX_test with logreg and store the results in y_pred.\n",
    "Use confusion_matrix() with y_test and y_pred to print the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c9944-9811-4868-a652-7a78bf903538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce93fcbd-3615-402e-af6a-8a22af43e3ad",
   "metadata": {},
   "source": [
    "### 6. Hyperparameter search and making the model perform better\n",
    "\n",
    "Create tol list with values 0.01, 0.001, and 0.0001, and max_iter list with values 100, 150, and 200.\n",
    "Create a param_grid dictionary with keys tol and max_iter, mapping them to their respective lists of values.\n",
    "Instantiate GridSearchCV() with a 5-fold cross-validation.\n",
    "Fit rescaledX_train and y_train to grid_model, storing results in grid_model_result.\n",
    "Store the best model from grid_model_result in best_model, the best model parameters in best_params, and best performance score in best_score.\n",
    "Evaluate the best model from grid_model_result on the test set (rescaledX_test, y_test).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c87481-69fc-4d9a-982c-c9e55f4978ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6810cc7-0e05-4480-9bcc-aaf507590c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2069de59-91d9-44ac-a401-f60c12465f24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85f96f-e45e-4c63-bdd7-f2e658ccec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the features 11 and 13\n",
    "cc_apps = cc_apps.drop([11, 13], axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "cc_apps_train, cc_apps_test = train_test_split(cc_apps, test_size=0.33, random_state=42)\n",
    "\n",
    "# Replace the '?'s with NaN in the train and test sets\n",
    "cc_apps_train_nans_replaced = cc_apps_train.replace(\"?\", np.NaN)\n",
    "cc_apps_test_nans_replaced = cc_apps_test.replace(\"?\", np.NaN)\n",
    "\n",
    "# Impute the missing values with mean imputation\n",
    "cc_apps_train_imputed = cc_apps_train_nans_replaced.fillna(cc_apps_train_nans_replaced.mean())\n",
    "cc_apps_test_imputed = cc_apps_test_nans_replaced.fillna(cc_apps_train_nans_replaced.mean())\n",
    "\n",
    "# Iterate over each column of cc_apps_train_imputed\n",
    "for col in cc_apps_train_imputed.columns:\n",
    "    # Check if the column is of object type\n",
    "    if cc_apps_train_imputed[col].dtypes == \"object\":\n",
    "        # Impute with the most frequent value\n",
    "        cc_apps_train_imputed = cc_apps_train_imputed.fillna(\n",
    "            cc_apps_train_imputed[col].value_counts().index[0]\n",
    "        )\n",
    "        cc_apps_test_imputed = cc_apps_test_imputed.fillna(\n",
    "            cc_apps_train_imputed[col].value_counts().index[0]\n",
    "        )\n",
    "\n",
    "# Convert the categorical features in the train and test sets independently\n",
    "cc_apps_train_cat_encoding = pd.get_dummies(cc_apps_train_imputed)\n",
    "cc_apps_test_cat_encoding = pd.get_dummies(cc_apps_test_imputed)\n",
    "\n",
    "# Reindex the columns of the test set aligning with the train set\n",
    "cc_apps_test_cat_encoding = cc_apps_test_cat_encoding.reindex(\n",
    "    columns=cc_apps_train_cat_encoding.columns, fill_value=0\n",
    ")\n",
    "\n",
    "# Segregate features and labels into separate variables\n",
    "X_train, y_train = (\n",
    "    cc_apps_train_cat_encoding.iloc[:, :-1].values,\n",
    "    cc_apps_train_cat_encoding.iloc[:, [-1]].values,\n",
    ")\n",
    "X_test, y_test = (\n",
    "    cc_apps_test_cat_encoding.iloc[:, :-1].values,\n",
    "    cc_apps_test_cat_encoding.iloc[:, [-1]].values,\n",
    ")\n",
    "\n",
    "# Instantiate MinMaxScaler and use it to rescale X_train and X_test\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX_train = scaler.fit_transform(X_train)\n",
    "rescaledX_test = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate a LogisticRegression classifier with default parameter values\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit logreg to the train set\n",
    "logreg.fit(rescaledX_train, y_train)\n",
    "\n",
    "# Use logreg to predict instances from the test set and store it\n",
    "y_pred = logreg.predict(rescaledX_test)\n",
    "\n",
    "# Print the confusion matrix of the logreg model\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Define the grid of values for tol and max_iter\n",
    "tol = [0.01, 0.001, 0.0001]\n",
    "max_iter = [100, 150, 200]\n",
    "\n",
    "# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\n",
    "param_grid = dict(tol=tol, max_iter=max_iter)\n",
    "\n",
    "# Instantiate GridSearchCV with the required parameters\n",
    "grid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit grid_model to the data\n",
    "grid_model_result = grid_model.fit(rescaledX_train, y_train)\n",
    "\n",
    "# Summarize results\n",
    "best_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\n",
    "print(\"Best: %f using %s\" % (best_score, best_params))\n",
    "\n",
    "# Extract the best model and evaluate it on the test set\n",
    "best_model = grid_model_result.best_estimator_\n",
    "print(\n",
    "    \"Accuracy of logistic regression classifier: \",\n",
    "    best_model.score(rescaledX_test, y_test),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
